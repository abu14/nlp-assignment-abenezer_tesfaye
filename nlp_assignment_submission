{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"06579399-5633-4bf4-b6be-542cacc960e6","cell_type":"markdown","source":"# Natural Language Processing (NLP) Assignment\nThis assignment will guide you through the basic concepts of Natural Language Processing including:\n- Text preprocessing\n- Tokenization and N-grams\n- Named Entity Recognition (NER)\n- Converting text into numbers (vectorization)\n- Word embeddings (for experienced learners)\n\nYou can run and modify the code cells below to complete the tasks.","metadata":{}},{"id":"ee10639b-cf09-428e-bff7-cda51b75a28a","cell_type":"code","source":"# Import required libraries\nimport nltk\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport numpy as np\nimport pandas as pd\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import ngrams\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:35:21.356089Z","iopub.execute_input":"2025-05-17T08:35:21.356399Z","iopub.status.idle":"2025-05-17T08:35:36.895092Z","shell.execute_reply.started":"2025-05-17T08:35:21.356373Z","shell.execute_reply":"2025-05-17T08:35:36.893863Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"id":"fbeb9c04-cf34-456b-8009-372cddc9e781","cell_type":"markdown","source":"## 1. Text Preprocessing\nClean the following text by converting it to lowercase, removing punctuation and stop words.","metadata":{}},{"id":"8516ef04-b88e-472b-b115-c7f55f45a185","cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Natural Language Processing is a fascinating field. It combines linguistics and computer science!\"\n\ndef preprocess(text):\n    # Convert to lowercase\n    # Tokenize\n    # Remove punctuation and stopwords\n    text = text.lower()\n    text = word_tokenize(text)   \n    stop_words = stopwords.words('english')\n    filtered_tok = [word for word in text if not word in stop_words]\n    text = ' '.join(filtered_tok)\n    text = re.sub('[^a-zA-Z ]+', '', text)\n    return text\n\n#final output \ncleaned_tokens = preprocess(text)\nprint(cleaned_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:13:38.623190Z","iopub.execute_input":"2025-05-17T09:13:38.623513Z","iopub.status.idle":"2025-05-17T09:13:38.631573Z","shell.execute_reply.started":"2025-05-17T09:13:38.623490Z","shell.execute_reply":"2025-05-17T09:13:38.630278Z"}},"outputs":[{"name":"stdout","text":"natural language processing fascinating field  combines linguistics computer science \n","output_type":"stream"}],"execution_count":33},{"id":"2ba369e0-5a23-41eb-a6ea-21febd1868e9","cell_type":"markdown","source":"## 2. Tokenization and N-grams\nGenerate bigrams (2-grams) from the cleaned tokens.","metadata":{}},{"id":"5d133582-ed2b-45a3-8366-eb09f4c21c45","cell_type":"code","source":"# Generate bigrams from cleaned tokens\nbigrams = list(ngrams(cleaned_tokens, 2))\nprint(\"Bigrams:\", bigrams)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:14:24.745678Z","iopub.execute_input":"2025-05-17T09:14:24.747746Z","iopub.status.idle":"2025-05-17T09:14:24.754992Z","shell.execute_reply.started":"2025-05-17T09:14:24.747710Z","shell.execute_reply":"2025-05-17T09:14:24.754018Z"}},"outputs":[{"name":"stdout","text":"Bigrams: [('n', 'a'), ('a', 't'), ('t', 'u'), ('u', 'r'), ('r', 'a'), ('a', 'l'), ('l', ' '), (' ', 'l'), ('l', 'a'), ('a', 'n'), ('n', 'g'), ('g', 'u'), ('u', 'a'), ('a', 'g'), ('g', 'e'), ('e', ' '), (' ', 'p'), ('p', 'r'), ('r', 'o'), ('o', 'c'), ('c', 'e'), ('e', 's'), ('s', 's'), ('s', 'i'), ('i', 'n'), ('n', 'g'), ('g', ' '), (' ', 'f'), ('f', 'a'), ('a', 's'), ('s', 'c'), ('c', 'i'), ('i', 'n'), ('n', 'a'), ('a', 't'), ('t', 'i'), ('i', 'n'), ('n', 'g'), ('g', ' '), (' ', 'f'), ('f', 'i'), ('i', 'e'), ('e', 'l'), ('l', 'd'), ('d', ' '), (' ', ' '), (' ', 'c'), ('c', 'o'), ('o', 'm'), ('m', 'b'), ('b', 'i'), ('i', 'n'), ('n', 'e'), ('e', 's'), ('s', ' '), (' ', 'l'), ('l', 'i'), ('i', 'n'), ('n', 'g'), ('g', 'u'), ('u', 'i'), ('i', 's'), ('s', 't'), ('t', 'i'), ('i', 'c'), ('c', 's'), ('s', ' '), (' ', 'c'), ('c', 'o'), ('o', 'm'), ('m', 'p'), ('p', 'u'), ('u', 't'), ('t', 'e'), ('e', 'r'), ('r', ' '), (' ', 's'), ('s', 'c'), ('c', 'i'), ('i', 'e'), ('e', 'n'), ('n', 'c'), ('c', 'e'), ('e', ' ')]\n","output_type":"stream"}],"execution_count":34},{"id":"fb037acd-f5dc-4525-b93d-b4dade8313de","cell_type":"markdown","source":"## 3. Named Entity Recognition (NER)\nUse spaCy to perform NER on a new sentence.","metadata":{}},{"id":"ed1da43e-7224-42b1-8e4c-8519ab3092dc","cell_type":"code","source":"# Example sentence\nsentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\ndoc = nlp(sentence)\nfor ent in doc.ents:\n    print(ent.text, ent.label_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:14:50.792654Z","iopub.execute_input":"2025-05-17T09:14:50.792985Z","iopub.status.idle":"2025-05-17T09:14:50.886723Z","shell.execute_reply.started":"2025-05-17T09:14:50.792964Z","shell.execute_reply":"2025-05-17T09:14:50.885389Z"}},"outputs":[{"name":"stdout","text":"Barack Obama PERSON\nHawaii GPE\n2008 DATE\n","output_type":"stream"}],"execution_count":35},{"id":"ff0f2632-0871-4ed8-801f-e597a7f8b3e1","cell_type":"markdown","source":"## 4. Converting Text to Numbers\nUse CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors.","metadata":{}},{"id":"98dcd331-ad16-4187-898d-d75305fafc0c","cell_type":"code","source":"sentences = [\n    \"I love machine learning.\",\n    \"Natural language processing is a part of AI.\",\n    \"AI is the future.\"\n]\n\n# CountVectorizer\ncount_vec = CountVectorizer()\nX_count = count_vec.fit_transform(sentences)\nprint(\"Count Vectorizer Output:\\n\", X_count.toarray())\n\n# TfidfVectorizer\ntfidf_vec = TfidfVectorizer()\nX_tfidf = tfidf_vec.fit_transform(sentences)\nprint(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:15:01.436199Z","iopub.execute_input":"2025-05-17T09:15:01.436532Z","iopub.status.idle":"2025-05-17T09:15:01.471614Z","shell.execute_reply.started":"2025-05-17T09:15:01.436509Z","shell.execute_reply":"2025-05-17T09:15:01.470375Z"}},"outputs":[{"name":"stdout","text":"Count Vectorizer Output:\n [[0 0 0 0 1 1 1 0 0 0 0 0]\n [1 0 1 1 0 0 0 1 1 1 1 0]\n [1 1 1 0 0 0 0 0 0 0 0 1]]\n\nTF-IDF Vectorizer Output:\n [[0.         0.         0.         0.         0.57735027 0.57735027\n  0.57735027 0.         0.         0.         0.         0.        ]\n [0.30650422 0.         0.30650422 0.40301621 0.         0.\n  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n [0.42804604 0.5628291  0.42804604 0.         0.         0.\n  0.         0.         0.         0.         0.         0.5628291 ]]\n","output_type":"stream"}],"execution_count":36},{"id":"659016c1-af08-4e7b-8e05-578cc5679206","cell_type":"markdown","source":"## 5. Word Embeddings (Advanced)\nUse spaCy to get word vectors (embeddings) for given words.","metadata":{}},{"id":"6a7a469a-2e87-4dc3-94a4-58774e590959","cell_type":"code","source":"word = nlp(\"machine\")[0]\nprint(\"Vector for 'machine':\\n\", word.vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:15:54.519923Z","iopub.execute_input":"2025-05-17T09:15:54.520306Z","iopub.status.idle":"2025-05-17T09:15:54.535025Z","shell.execute_reply.started":"2025-05-17T09:15:54.520283Z","shell.execute_reply":"2025-05-17T09:15:54.534196Z"}},"outputs":[{"name":"stdout","text":"Vector for 'machine':\n [-0.7506131  -0.57648134  0.64351434  0.34771815  0.45008683 -0.31984502\n  1.3374304   0.6823808  -0.2497879   0.01502723  0.20069116 -0.53005815\n -0.32142693  0.6083895   0.5911227   1.3969526  -1.3394687  -0.49667895\n  0.93146133  0.76211375 -0.63203835  1.1820786  -0.8377956   0.02632815\n -0.2938518   0.6069318   1.5544686  -0.04658484 -0.4521918   0.48126173\n  0.02117339  0.9538507   0.38607836  0.03060612 -1.2614324  -0.71200204\n -0.0582068   0.9979756   0.39407492  0.03983042 -0.9099759  -0.30680424\n  0.81676024  0.40132928 -0.6591901  -0.52833277 -0.1020698  -0.39648485\n -0.2746659  -0.5868281   0.11670423 -0.02715784  0.10342616 -0.71523666\n  0.78196347  0.26182324  1.2007883   0.40819865 -0.81201667  0.10142356\n -0.92464274 -0.0610747  -0.28506368 -0.27212036 -0.06658131  0.21739644\n -0.34570417 -0.7191127  -0.6493219  -0.07578599 -0.26895642  0.25262332\n  0.8506174   0.5730195  -0.10925089 -0.48964912 -0.33062595 -0.6904906\n  0.58739054  0.52517235 -0.47667012 -0.37759903 -0.88804555 -0.58199006\n  0.22513609  1.8321126  -0.54407513  0.07091011 -1.0318854   0.15322405\n -0.44268894  0.00544617  0.5723448   0.2448978  -0.8239452   0.42069194]\n","output_type":"stream"}],"execution_count":37},{"id":"0573a09d-37a5-430d-99cb-53ea3d5358b2","cell_type":"markdown","source":"### **Crate Vector Store**","metadata":{}},{"id":"204655a1-e008-4167-ac2c-9406041dc1af","cell_type":"code","source":"#crete vector store for cleaned text\nvector_store = {}\n\nfor text in cleaned_tokens:\n    doc = nlp(cleaned_tokens)\n    vector_store[cleaned_tokens] = doc.vector\n\n\nvector_store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:35:03.561726Z","iopub.execute_input":"2025-05-17T09:35:03.562039Z","iopub.status.idle":"2025-05-17T09:35:04.184069Z","shell.execute_reply.started":"2025-05-17T09:35:03.562016Z","shell.execute_reply":"2025-05-17T09:35:04.183017Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'natural language processing fascinating field  combines linguistics computer science ': array([ 2.20488161e-01, -3.78783762e-01, -6.09863810e-02,  6.35852963e-02,\n        -7.90024400e-02, -7.49741644e-02,  8.71399045e-01,  4.12209839e-01,\n         2.71053016e-01, -4.65288490e-01, -3.74556661e-01, -1.50885612e-01,\n        -1.47162691e-01, -2.32999206e-01,  1.38312444e-01,  1.01389624e-01,\n        -3.39223325e-01, -2.17681095e-01, -1.21847570e-01, -2.86937982e-01,\n        -7.42743686e-02,  6.84384227e-01, -2.22558588e-01,  2.22039074e-01,\n         5.91395982e-02, -3.84523481e-01, -2.75105231e-05,  4.02710021e-01,\n         3.88214290e-01, -1.98545262e-01,  2.12986730e-02,  6.19576462e-02,\n        -2.16640070e-01, -2.49977902e-01, -1.31761655e-01, -1.91126034e-01,\n        -7.95000643e-02,  4.51175682e-02,  1.68184668e-01, -1.22696377e-01,\n        -3.28380257e-01,  6.57415509e-01,  1.45283818e-01,  3.28659236e-01,\n         3.42066586e-01,  1.45150691e-01, -3.43575418e-01,  2.18365863e-01,\n         5.25557995e-02, -2.66606510e-01, -2.70556837e-01,  1.64110959e-01,\n        -8.01081955e-03, -3.46852154e-01, -2.14066073e-01, -1.41435608e-01,\n        -9.05532688e-02,  2.23043203e-01, -2.20893189e-01, -7.56916851e-02,\n        -5.35903573e-01,  2.28054971e-01, -1.11640669e-01, -1.18144967e-01,\n        -2.20530238e-02, -9.59451422e-02,  1.75536126e-01,  3.25594723e-01,\n         2.26591781e-01, -3.29726756e-01,  5.56120813e-01,  1.10018149e-01,\n         7.96755180e-02, -2.79354870e-01,  9.49457526e-01, -3.46644700e-01,\n        -8.28665048e-02, -4.64901626e-01,  4.61211383e-01, -3.31212580e-01,\n        -3.33237946e-01,  5.35633564e-02, -5.56735620e-02, -2.93736875e-01,\n        -5.39427027e-02,  1.49091035e-01,  2.27414966e-01, -4.06314939e-01,\n        -4.92385387e-01,  1.56232268e-01, -1.80845663e-01,  3.43241155e-01,\n         2.92398840e-01,  4.75436211e-01, -4.25480187e-01, -2.03763679e-01],\n       dtype=float32)}"},"metadata":{}}],"execution_count":43}]}